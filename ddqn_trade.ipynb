{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LR = 0.01\n",
    "MEMORY_LEN = 3000\n",
    "GAMMA = 0.9\n",
    "EPSILON = 0.9\n",
    "EPOCH = 100\n",
    "UPDATE_FREQUENCY = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 59.799999  ,  60.400002  ,  59.299999  , ...,  59.44179856,\n",
       "          -0.69113272,  -0.87297582]],\n",
       "\n",
       "       [[ 60.799999  ,  61.        ,  60.5       , ...,  59.65075247,\n",
       "          -0.53159123,  -0.8046989 ]],\n",
       "\n",
       "       [[ 60.5       ,  61.        ,  60.200001  , ...,  59.85832902,\n",
       "          -0.38458182,  -0.72067549]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[294.        , 294.        , 291.5       , ..., 294.79525067,\n",
       "           0.79173718,   1.7364945 ]],\n",
       "\n",
       "       [[293.        , 294.        , 291.        , ..., 294.67290441,\n",
       "           0.66965118,   1.52312584]],\n",
       "\n",
       "       [[296.        , 298.        , 295.        , ..., 295.10784219,\n",
       "           0.84557069,   1.38761481]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_path = \"/Users/ted/Desktop/lab/2330Inf.csv\"\n",
    "test_path = \"/Users/ted/Desktop/lab/3034Inf.csv\"\n",
    "stock_df = pd.read_csv(stock_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "stock_df2 = pd.DataFrame(stock_df,columns=[\"Open\",\"High\",\"Low\",\"Close\",\"K value\",\"D value\",\"EMA 12\",\"DIF\",\"MACD\"])\n",
    "test_df2 = pd.DataFrame(stock_df,columns=[\"Open\",\"High\",\"Low\",\"Close\",\"K value\",\"D value\",\"EMA 12\",\"DIF\",\"MACD\"])\n",
    "# print(stock_df2)\n",
    "\n",
    "# declear training features data\n",
    "features = []\n",
    "for i in range(1,len(stock_df2)):\n",
    "    x = stock_df2[i-1:i][[\"Open\",\"High\",\"Low\",\"Close\",\"K value\",\"D value\",\"EMA 12\",\"DIF\",\"MACD\"]].values\n",
    "    features.append(x.tolist())\n",
    "features = np.array(features)\n",
    "# print(\"size of feature: \",features.size())\n",
    "# print(features[0].shape)\n",
    "\n",
    "\n",
    "# declear trainging labels data\n",
    "test_features = []\n",
    "for i in range(1,len(test_df2)):\n",
    "    x = stock_df2[i-1:i][[\"Open\",\"High\",\"Low\",\"Close\",\"K value\",\"D value\",\"EMA 12\",\"DIF\",\"MACD\"]].values\n",
    "    test_features.append(x.tolist())\n",
    "test_features = np.array(test_features)\n",
    "#print(\"size of feature: \",test_features.size())\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    #inital environment \n",
    "    def __init__(self,env1):\n",
    "        self.day = 0             \n",
    "        self.env1 = env1         # size:  (2254 , 30 , 6)\n",
    "        self.stockNum = 0        # num of stock\n",
    "        self.money = 10000       # initial budget\n",
    "        \n",
    "    # reset environment  , return first state\n",
    "    def reset(self):\n",
    "        self.day = 0             \n",
    "        self.stockNum = 0       \n",
    "        self.money = 10000\n",
    "        return self.env1[0]      # first state\n",
    "    \n",
    "    # return n-day's state\n",
    "    def get_observation(self):\n",
    "        return self.env1[self.day] \n",
    "    \n",
    "    #return action space [ sell, hold , buy] \n",
    "    def get_actions(self,n):\n",
    "        a = [\"sell\",\"hold\",\"buy\"]\n",
    "        return a[n] # [sell,hold,buy]\n",
    "    \n",
    "    # if finish or not\n",
    "    def is_done(self):\n",
    "        return self.day == 2445\n",
    "    \n",
    "    \n",
    "    # input : action  output :　reward    \n",
    "    def actions(self,action,date):\n",
    "        self.day = date\n",
    "        reward = 0\n",
    "        if(action == 0):     # sell\n",
    "            self.money += (self.stockNum*self.env1[self.day][0][3])   #  sell all stock\n",
    "            self.stockNum = 0\n",
    "            reward = self.env1[self.day][0][3] - self.env1[self.day+1][0][3]   #calculate reward\n",
    "        elif(action == 1):   #hold\n",
    "            reward =  -1* self.env1[self.day+1][0][3]\n",
    "        elif(action == 2):   #buy\n",
    "            num = self.money // self.env1[self.day][0][3]\n",
    "            self.stockNum += num                             # buy as many as stock as agent can\n",
    "            self.money -= (num * self.env1[self.day][0][3])\n",
    "            reward = self.env1[self.day+1][0][3] - self.env1[self.day][0][3]           #calculate reward\n",
    "        self.day += 1          # next day\n",
    "        return reward\n",
    "        \n",
    "        \n",
    "#     def step(self,act_probs):\n",
    "        \n",
    "#         act_probs = act_probs.data.numpy()[0]                    # get the probability of each action (sell , hold ,buy)\n",
    "#         action = np.random.choice(len(act_probs), p=act_probs)   # random choose action but the action with higher probability \n",
    "#                                                                  # would have higher chance to be choose\n",
    "#         if(action == 0 ):  #sell                                   \n",
    "#             if(self.stockNum == 0):                              # if agent don't have any stock to sell\n",
    "#                 if(self.money >= self.close[self.day]):          # if agent have enough budget for buying stocks \n",
    "#                     space = [1,2]                   \n",
    "#                     action = np.random.choice(space)             # random choose buy or hold\n",
    "#                 else:\n",
    "#                     action = 1                                   # otherwise choose hold\n",
    "                    \n",
    "#         elif(action == 2): # buy\n",
    "#             if(self.money < self.close[self.day]):               # if agent don't have enough meney for buying stocks\n",
    "#                 if(self.stockNum != 0):                          # if agent have one or more stock to sell\n",
    "#                     space = [0,1]\n",
    "#                     action = np.random.choice(space)             # random choose sell or hold\n",
    "#                 else:\n",
    "#                     action = 1                                   # otherwise choose hold\n",
    "                    \n",
    "#         #print(\"Day %d , Action: %s\"%(self.day ,self.get_actions(action)),end = ' ')\n",
    "#         reward = self.actions(action)                            # get reward\n",
    "#         #print(\"Reward: %.4f , StockNum: %d ,Budget: %.4f\"%(reward,self.stockNum,self.money))\n",
    "        \n",
    "#         if(self.is_done()):                                      # if finish , print the rate of return\n",
    "#             print(\"---------------------------------------------\")\n",
    "#             print(\"Total Reward: %lf\" % (((self.stockNum * self.close[self.day]) + self.money)*100/10000))\n",
    "#             print(\"---------------------------------------------\")\n",
    "#         return self.get_observation(), reward, self.is_done(),action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 32)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.fc2 = nn.Linear(32,64)\n",
    "        self.fc2.weight.data.normal_(0, 0.1)\n",
    "        self.out = nn.Linear(64, 3)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN():\n",
    "    def __init__(self):\n",
    "        self.eval_net,self.target_net = Net(),Net()\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)    \n",
    "#         self.loss_func = nn.MSELoss()\n",
    "        self.memory_counter = 0\n",
    "        self.memory = np.zeros((MEMORY_LEN, 20))\n",
    "#         self.memory = np.zeros((MEMORY_LEN,MEMORY_LEN))\n",
    "        \n",
    "        \n",
    "    def store_memory(self,s,a,r,s_):\n",
    "        index = self.memory_counter % MEMORY_LEN\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        self.memory[index,:] = transition\n",
    "        self.memory_counter += 1\n",
    "    \n",
    "    def train(self,loss):\n",
    "        self.optimaizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def learn(): #更新兩個network的參數\n",
    "#         sample_index = np.random.choice(MEMORY_LEN,BATCH_SIZE)\n",
    "#         b_memory = self.memory[sample_index,:]\n",
    "#         b_r = torch.FloatTensor(b_memory[])\n",
    "        self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        \n",
    "    def choose_action(self,x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        if np.random.uniform() <= EPSILON:   # greedy\n",
    "            target_value = self.eval_net.forward(x)\n",
    "            if(target_value[0][0]>target_value[0][1] and target_value[0][0]>target_value[0][2]):\n",
    "                return 0\n",
    "            if(target_value[0][1]>target_value[0][2] and target_value[0][1]>target_value[0][0]):\n",
    "                return 1\n",
    "            if(target_value[0][2]>target_value[0][1] and target_value[0][2]>target_value[0][0]):\n",
    "                return 2\n",
    "#             action = torch.max(actions_value, 1)    # return the argmax\n",
    "        else:   # non-greedy\n",
    "            action = np.random.randint(1,size=(0,2))\n",
    "        return action\n",
    "    def eval_Qtarget(self,x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x),0)\n",
    "        target_value = self.target_net(x)\n",
    "#         print(target_value[0])\n",
    "#         if(target_value[0]>target_value[1] and target_value[0]>target_value[2]):\n",
    "#             return target_value[0]\n",
    "#         if(target_value[1]>target_value[2] and target_value[1]>target_value[0]):\n",
    "#             return target_value[1]\n",
    "#         if(target_value[2]>target_value[1] and target_value[2]>target_value[0]):\n",
    "#             return target_value[2]\n",
    "#         target_value_ = torch.max(target_value,1)\n",
    "#         target = target_value_.item()\n",
    "#         target_value = torch.FloatTensor(target_value_)\n",
    "        return target_value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batch(env,net,batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    \n",
    "    obs = env.reset()        # initialize enveriment\n",
    "    \n",
    "    sm = nn.Softmax(dim = 1)\n",
    "    \n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor(obs)      # numpy to tensor\n",
    "        obs_v = obs_v.view(-1,30,6)         # reshape to (batch_size, time_steps , input_size)\n",
    "        out = net(obs_v)\n",
    "        act_probs = sm(out)                 # calculate the probability of each action\n",
    "    \n",
    "        next_obs , reward , is_done,action = env.step(act_probs)  # do the action , return (next state , reward , is_done , action)\n",
    "        \n",
    "        episode_reward += reward            # calculate total of reward\n",
    "        episode_steps.append(EpisodeStep(observation = obs,action = action))\n",
    "        \n",
    "        if is_done:                         # if finish one round\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()          # initialize environment\n",
    "            if len(batch) == batch_size:    # if finish all rounds\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddqn = DDQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(features)\n",
    "obs_size = 9\n",
    "n_actions = 3\n",
    "for i in range(EPOCH):\n",
    "    #initialize sequence s1 = {x1} and preprocessed sequenced\n",
    "    for j in range(len(features)-1):\n",
    "        state = features[j][0]\n",
    "        state_ = features[j+1][0]\n",
    "        Q_value = ddqn.choose_action(state)\n",
    "        Q_value_next = ddqn.choose_action(state_)\n",
    "        Q_value_ = ddqn.eval_Qtarget(state_)\n",
    "#         print(Q_value_)\n",
    "# #         Q_value_ = Q_value_.item()\n",
    "# #         Q_value_next = float(Q_value_next)\n",
    "# #         Q_target = Q_value_next + GAMMA * Q_value_\n",
    "#         reward = env.actions(Q_value,j)\n",
    "#         # action,reward,state,next_state\n",
    "# #         ddqn.store_memory(state,Q_value,reward,state_)\n",
    "#         if(j%UPDATE_FREQUENCY==0):\n",
    "#             pass\n",
    "# #           ddqn.learn()\n",
    "# #         print(action)\n",
    "# #         loss = loss_func(Q_value,Q_target)\n",
    "# #         train(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
